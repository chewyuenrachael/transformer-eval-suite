# ğŸ§  Transformer Evaluation Suite

A developer-facing benchmarking and visualization toolkit to evaluate ğŸ¤— Hugging Face and OpenAI transformer models using prompt variants and core NLP metrics.

[ğŸš€ **Live Demo**](https://transformer-eval-suite.streamlit.app/)  
_Explore the app live in your browser â€” compare models, prompts, and metrics interactively._

---

## ğŸ§­ Whatâ€™s New

âœ… **OpenAI API Integration**  
Use models like `gpt-4o`, `gpt-4`, and `gpt-3.5-turbo` with your API key via `.env`.

âœ… **Hugging Face Dataset Integration**  
Now supports datasets from ğŸ¤— `datasets` like `FreedomIntelligence/medical-o1-reasoning-SFT` to evaluate on real-world, medically grounded questions.

âœ… **Evaluation Filtering**  
Run evaluation only on shorter examples using `--filter-short` or modify filtering logic in `cli.py`.

---

## ğŸ” What It Does

This suite lets you:

- Choose **any transformer model** from Hugging Face **or OpenAI**
- Benchmark them across different **prompt formulations** for a given task (e.g., summarization)
- Load **domain-specific datasets** for evaluation (e.g., medical reasoning)
- Evaluate generated outputs using:
  - **BLEU**: n-gram precision
  - **ROUGE-1 / ROUGE-L**: recall and LCS overlap
  - **Cosine Similarity**: using `sentence-transformers`
- Export to **CSV** for reporting
- Visualize results using **Plotly** or a **Streamlit dashboard**

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/YOUR_USERNAME/transformer-eval-suite.git
cd transformer-eval-suite

# Set up virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

> For Hugging Face dataset access:  
```bash
huggingface-cli login
```

> For OpenAI model access:  
```bash
echo "OPENAI_API_KEY=your-key-here" > .env
```

---

## ğŸ”§ Running Evaluations

### âœ… Hugging Face models
```bash
python cli.py --models gpt2 EleutherAI/gpt-neo-125M --task summarization
```

### âœ… OpenAI models
```bash
python cli.py --models openai/gpt-4o --task summarization
```

> This will:
- Pull real medical reasoning prompts from Hugging Face
- Run 3 prompt variants per example
- Evaluate outputs with BLEU, ROUGE, and Cosine Similarity
- Save results to `results/summarization-comparison/`

---

## ğŸ§ª Evaluation Metrics

| Metric | Description |
|--------|-------------|
| BLEU | Precision of n-grams |
| ROUGE-1 | Overlap of unigrams |
| ROUGE-L | Longest common subsequence |
| Cosine | Semantic similarity via `sentence-transformers` |

---

## ğŸ“ Output Structure

```
results/
â”œâ”€â”€ summarization-comparison/
â”‚   â”œâ”€â”€ openai_gpt-4o.json
â”‚   â”œâ”€â”€ all_results.json
â”‚   â”œâ”€â”€ results.csv
```

---

## ğŸ©º Built-in Dataset (Optional)

This project currently loads:

ğŸ“š `FreedomIntelligence/medical-o1-reasoning-SFT`  
A high-quality dataset for advanced medical reasoning (~25k examples).  
To modify, edit this section in `cli.py` or `runner.py`:

```python
dataset = load_dataset("FreedomIntelligence/medical-o1-reasoning-SFT", "en")["train"]
dataset = dataset.filter(lambda x: len(x["Question"]) < 500 and len(x["Response"]) < 800)
```

---

## ğŸ“Š Visualization Options

### ğŸ“ˆ CLI Charts
```bash
python plot_results.py --metric BLEU --show
```

### ğŸŒ Streamlit Dashboard
```bash
streamlit run dashboard.py
```

Then open `http://localhost:8501`

---

## âœ¨ Sample Evaluation Table

| Model         | Prompt | Cosine Sim | BLEU  | ROUGE-1 | ROUGE-L |
|---------------|--------|------------|-------|----------|----------|
| openai/gpt-4o | 0      | 0.815      | 0.148 | 0.493    | 0.313    |
| openai/gpt-4o | 1      | 0.884      | 0.200 | 0.630    | 0.396    |
| openai/gpt-4o | 2      | 0.904      | 0.117 | 0.535    | 0.323    |

---

## ğŸ§  Prompt Templates

The suite tests each model on prompt styles like:

- `Summarize the following text:`
- `Can you provide a summary of:`
- `TL;DR:`

This reveals **prompt sensitivity** and helps optimize instruction design.

---

## âœ… Use Cases

- ğŸ”¬ Research into LLM capabilities and prompt sensitivity
- ğŸ› ï¸ Developer tools for prompt + model comparison
- ğŸ§ª QA + summarization pipelines
- ğŸ§­ OpenAI API-based evaluation harness

---

## ğŸ“Œ To-Do

- [ ] Expand metrics (BERTScore, METEOR)
- [ ] Add classification / QA tasks
- [ ] Integrate with Claude / Anthropic API
- [ ] Save prompt-output pairs for ablation
- [ ] Add more domain-specific datasets

---

## ğŸ“„ License

MIT License

---

## ğŸ‘©â€ğŸ’» Author

**Rachael Chew**  
AI systems | Developer tooling | Medical cognition  
GitHub: [@chewyuenrachael](https://github.com/chewyuenrachael)

---

## ğŸ™Œ Contributing

Pull requests welcome!  
Want to add datasets, metrics, or visualization tools? Letâ€™s build together.